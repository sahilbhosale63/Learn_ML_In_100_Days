# Learn_ML_In_100_Days
Learn Machine Learning #100DaysOfMLCode
https://sahilbhosale63.github.io/Learn_ML_In_100_Days/

# Day 01

**Today's Progress:** I refreshed the basics topics of linear algebra like vectors, linear combination, linear transformation, matrix multipication and determinants from the 3Blue1Brown youtube channel.

**Thought:** I would be enhancing my maths skills first and then start with the actual machine learning concepts. 

# Day 02

**Today's Progress:** Learned the concepts of linear algebra like Inverse matrix, column space, null space, non-square matrix, dot product and cross product from the 3Blue1Brown youtube channel.

**Thought:** Currently I am gaining the basic understanding of the topics in linear algebra and after this I will be diving deep into these topics.


# Day 03

**Today's Progress:** Learned the concepts of linear algebra like cross product in linear transformation, change in basis, Eignvalues and Eigenvectors and abstract vector space from the 3Blue1Brown youtube channel.

**Thought:** Completed the 3Blue1Brown videos and from tomorrow will learn this subject in detail.


# Day 04

**Today's Progress:** Completed the first 2 lectures from the MIT OpenCourseWare's [Linear Algebra](https://www.youtube.com/watch?v=ZK3O402wf1c&list=PLE7DDD91010BC51F8) Course and learned the following topics:

**Lecture 1:** N linear equations with n unknown,  Row picture , column picture, matrix form

**Lecture 2:** Elimination of matrix by row, back substitution, matrix multiplication, inverse of a matrix

**Thought:** Really understand this topics deeply as what matrix actually are and how to perform various operations on them. Looking forward to completed this course lectures in a week with proper understanding.


# Day 05

**Today's Progress:** Completed the 3rd & 4th lectures from the MIT OpenCourseWare's Linear Algebra course. Topics covered: Multiplication of matrix (4 ways), finding inverse of matrix i.e A^-1 by Gauss-Jordan Elimination method, Factorization into A = LU , Transpose of a matrix, Permutation matrix.

**Lecture 3:** Multiplication of matrix 4 ways: row *col , row * matrix, matrix * col, col * row( It's a special case ).
Inverse of a matrix ( A^-1 A = I ), if and only if A^-1 exist and finding inverse i.e A^-1 by gauss - Jordan Elimination method.
Also, A^-1 A = I = A A^-1, i.e A inverse * A is same as A * A inverse which is equal to identity matrix (I).

**Lecture 4:** Factorization of matrix also known as matrix decomposition Is a way of reducing a matrix into constituent parts and in the form A = LU ( where L is lower triangular matrix and U is upper triangular matrix). More details here: https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/

Inverse of two matrix i.e AB is B^-1A^-1 AB = I.
To find transpose of a matrix we make row as a column and vice-versa.
In permutation matrix we exchange rows. Also, In permutatin matrix Inverse of a matrix is equal to its transpose ( P^-1 = P^T).

**Thought:** Getting some more grip on this subject as I am moving along.


# Day 06

**Today's Progress:** Learned about vector spaces and subspaces, column space and null space in linear algebra from @MIT OpenCourseWare lecture 5 and lecture 6.

Vectors Space has to be closed under Multiplication and addition of vectors which means in other words linear combination

How to create a subspace from a matrix?
Take the columns of a matrix and take there combinations or there linear combinations and you get the column space.

**Thought:** completed the vector spaces part of linear algebra.


# Day 07

**Today's Progress:** Learned about computing the nullspaces, Echelon form, complete solution of AX = B and rank of a matrix from MIT lecture 7 and lecture 8.
 

**Thought:** learning some advance stuff and getting it to the middle of linear algebra course.


# Day 08

**Today's Progress:** Learned about the Linear independence, basics and dimensions, rowspaces and Nullspace of A^T from the lecture 9 and 10 of the MIT open courseware.

**Thought:** The things which I learned today was new to me and I had never learned this in my school. There can be some topics from linear algebra which you might not know, so I encourage you all if you are learning mathematics or ML please check out this MIT Linear Algebra course link. The link is mentioned in Day 04.  

# Day 09

**Today's Progress:** Day 09: learned about Bases of new vector spaces, rank one matrix and basics of graphs in linear algebra from @MIT courseware lecture 11.

**Thought:** Watched only one video today because I was working on my internship web project.


# Day 10

**Today's Progress:**  Learned about the Graphs & Networks, Incidence matrix, proved Kirchoff's law in terms of a matrix and solved some sums related to what I have learned so far from the MIT open courseware lecture 12 and 13.

**Thought:** After solving the kirchoff's law in term of matrix form I think linear algebra can do anything. :stuck_out_tongue_winking_eye: :heart_eyes:


# Day 11

**Today's Progress:** Learned about the orthogonal vectors, why nullspaces are perpendicular to rowspaces, Nullspace of 'A' transpose dot 'A' is equal to Nullspace of 'A', projections, least square and projection matrix from the MIT OCW.

Lecture 14: Orthogonal vectors, why nullspaces are perpendicular to rowspaces, Nullspace of 'A' transpose dot 'A' is equal to Nullspace of 'A' i.e N(A^T A) = N(A).

Lecture 15: Projections & why we use projections, least square and projection matrix.

**Thought:** Watched 2 video lectures today.

# Day 12

**Today's Progress:** Continued the projection matrix part and solved some sums related to that from MIT OCW lecture 16 of Linear Algebra.

**Thought:** today watched only one lecture.


# Day 13

**Today's Progress:** Learned about the Orthogonal basis, orthogonal matrix, Gram Schmidt method, determinants, 10 properties of determinants from Linear Algebra course.

Lecture 17: Orthogonal basis, orthogonal matrix, Gram Schmidt method.

Lecture 18: Determinants & 10 properties of determinants.

**Thought:** Understood the determinants part and its properites.


# Day 14

**Today's Progress:** Learned about the Formula for determinant of A, cofactor formula, tridiagonal matrices, formula for A^-1, Cramer's rule and volume of the box.

Lecture 19: Formula for determinant of A, cofactor formula, tridiagonal matrices.

Lecture 20: Formula for A^-1, Cramer's rule and volume of the box.

**Thought:** learned about the other formulas of the determinants.

# Day 15

**Today's Progress:** Learned about Eigen values and Eigen vectors, det [A - Î»I] = 0, diagonalization of a matrix from MIT linear algebra lecture 21 & 22.

**Thought:** Understanding the concepts.


# Day 16

 
**Today's Progress:** Learned about the Markov matrices, steady state, Fourier series of projections, differential equation of du/dr = Au and exponential e^At of a matrix for linear algebra MIT OCW lecture 23 & 24.


**Thought:** Also be reading the book of Linear Algebra by Prof. Gilbert Strange from tomorrow because i think it will be great to actually ready and revise those previous learned topics to understand the concepts better. As while watching a video there are topics which i didn't understand or i have some doubts related to a particular topic so that will be cleared through reading those topics again.


# Day 17

**Today's Progress:** Learned about symmetric matrices, hermitian matrices and positive definite matrices from lecture 25 of linear algebra.

**Thought:** Exploring & learning more concepts from linear algebra.


# Day 18

**Today's Progress:** Today learned about the Complex no's (in vectors & matrices), Fourier transform, fast Fourier transform, positive definite matrices (tests), test for minimum (X^TA > 0) and ellipsoids in R^n from lecture 26 &27 of MIT OCW linear algebra.

Lecture 26: This lectured covered Complex no's (in vectors & matrices), Fourier transform, fast Fourier transform. 

Lecture 27: This lectured covered positive definite matrices (tests), test for minimum (X^TA > 0) and ellipsoids in R^n.


**Thought:** Learning by proving the things.



# Day 19 

**Today's Progress:** Learned about the similar matrices (jordan form) and singular value decomposition (SVD) from mit linear algebra course.

Lecture 28: This lectured covered similar matrices (jordan form).

Lecture 29: This lectured covered singular value decomposition (SVD).

**Thought:** Today learned 2 topics in detail


# Day 20 

**Today's Progress:** Learned about linear transformation in linear algebra.

**Thought:** watched only a single video.



# Day 21 

**Today's Progress:** Learned about change of basis and its application (compression of images), left inverse, right inverse & pseudo inverse in linear algebra.

**Thought:** Finally completed the Linear Algebra part and will start with calculus from tomorrow.


# Day 22 

**Today's Progress:** Watched the first 5 videos of the 3Blue1Brown Essence of Calculus series.


**Thought:** Will try to complete this video series within the next two days. 




# Day 23 & 24

**Today's Progress:** Completed the 3Blue1Brown "Essence of Calculus" series. From tomorrow will start with probability & Statistic  according to @sirajraval ML curriculum.

**Thought:** completed the calculus.



# Day 25 

**Today's Progress:** Today started with probability and Learned about sample space, events, conditional probability, solved some examples on conditional probability and properties of conditional probability.


**Thought:** started with propability.



# Day 26 & 27 

**Today's Progress:** Learned about Multiplication theorem and independent events and solved sums related to that in probability.


**Thought:** completed only two topics as I also have to attend the college.



# Day 28 

**Today's Progress:** Today understood the concept of Total probability and solved sums related to it.

**Thought:** understanding the concepts.



# Day 29 

**Today's Progress:** Today learned about the Bayes' theorem in probability and solved sums related to it.

**Thought:** Understanding the theorems in probability.


# Day 30 

**Today's Progress:** Learned about random variables, Bernoulli random variables and binomial random variables.

**Thought:** Today started with random variables.


# Day 31 

**Today's Progress:** Poisson random variables, continuous and uniform random variables in probability theory.

**Thought:** some part of random variables is remaining will do it tomorrow.



# Day 32 

**Today's Progress:** Solved some sums related to uniform random variable in probability theory.

**Thought:** only solved sums today.


# Day 33 

**Today's Progress:** Learned about exponential random variable and normal random variable in probability theory.

**Thought:** Completed with random variables part.



# Day 34 

**Today's Progress:**  Learned about Expectation (Mean), Expectation of Bernoulli Random Variable in probability.

**Thought:** Started with Expectation i.e Mean.



# Day 35 

**Today's Progress:** Learned about Expectation of Binomial random variables and Poisson Random Variable in probability.

**Thought:** Will cover the remaining mean in upcoming days.





# Day 36 

**Today's Progress:** Learned about Expectation of poisson random variables, continuous Random Variable and uniform Random Variable in probability.


**Thought:** learned only three topics


# Day 37 & 38
**Today's Progress:** Learned about Expectation of exponential random variable, expectation of normal random variable, mean, median and mode in probability theory.


# Day 39 & 40 
**Today's Progress:** Learned about Inequalities - Markov inequalities and chebyshev inequalities, weak law of large numbers and polling in limits.


# Day 41 & 42: 
**Today's Progress:** Learned about convergence in probability with examples, Central limit theorem (CLT) with examples, illustration of CLT.


# Day 43 & 44: 
**Today's Progress:** Learned about the overview of classical statistics, mean squared error, confidence interval, confidence mean for unknown mean and for mean when variance is unknown and Maximum likelihood estimation.

# Day 45 & 46: 
**Today's Progress:** Learned about Bernoulli process, stochastic process, properties of Bernoulli process, merging and splitting of Bernoulli process and Poisson approximation to Bernoulli process from @edXOnline Intro to probability course.



# Day 47 & 48: 
**Today's Progress:** Learned about definition of poisson process and it's applications, mean and variance of no of arrivals, merging independent poison process and splitting poison process in probability theory.



# Day 49 & 50: 
**Today's Progress:** Finished the remaining topics of probability theory from edx intro to probability course.

Completed with the Mathematics Part ðŸ˜ŒðŸ¤—. Next will start with Python programming.ðŸ’»


# Day 51 & 52: 
**Today's Progress:** Started with python. Learned about variables and it's types, operations with variables, type conversion, what are lists, creating a list and list of lists.


# Day 53 & 54: 
**Today's Progress:** Learned about subsetting lists, subset and conquer, subset & calculate, slicing & dicing, subsetting list of lists, list manipulation, replace list items, extend list and delete list elements in python.


# Day 55 & 56: 
**Today's Progress:** Learned about functions, passing multiple argument, methods, string methods, list methods, packages, importing package, different ways of importing packages in Python.


# Day 57 & 58: 
**Today's Progress:** Started with Numpy library and created first Numpy array, subsetting Numpy array, 2d Numpy array, subsetting of 2d Numpy array, 2d arithmetic and Numpy basic statistics.


# Day 59 & 60: 
**Today's Progress:** Started with matplotlib and implemented line plot, scatter plot, histograms, made customization and interpretation to this plots.
