# Learn_ML_In_100_Days
Learn Machine Learning #100DaysOfMLCode
https://sahilbhosale63.github.io/Learn_ML_In_100_Days/

# Day 01

**Today's Progress:** I refreshed the basics topics of linear algebra like vectors, linear combination, linear transformation, matrix multipication and determinants from the 3Blue1Brown youtube channel.

**Thought:** I would be enhancing my maths skills first and then start with the actual machine learning concepts. 

# Day 02

**Today's Progress:** Learned the concepts of linear algebra like Inverse matrix, column space, null space, non-square matrix, dot product and cross product from the 3Blue1Brown youtube channel.

**Thought:** Currently I am gaining the basic understanding of the topics in linear algebra and after this I will be diving deep into these topics.


# Day 03

**Today's Progress:** Learned the concepts of linear algebra like cross product in linear transformation, change in basis, Eignvalues and Eigenvectors and abstract vector space from the 3Blue1Brown youtube channel.

**Thought:** Completed the 3Blue1Brown videos and from tomorrow will learn this subject in detail.


# Day 04

**Today's Progress:** Completed the first 2 lectures from the MIT OpenCourseWare's [Linear Algebra](https://www.youtube.com/watch?v=ZK3O402wf1c&list=PLE7DDD91010BC51F8) Course and learned the following topics:

**Lecture 1:** N linear equations with n unknown,  Row picture , column picture, matrix form

**Lecture 2:** Elimination of matrix by row, back substitution, matrix multiplication, inverse of a matrix

**Thought:** Really understand this topics deeply as what matrix actually are and how to perform various operations on them. Looking forward to completed this course lectures in a week with proper understanding.


# Day 05

**Today's Progress:** Completed the 3rd & 4th lectures from the MIT OpenCourseWare's Linear Algebra course. Topics covered: Multiplication of matrix (4 ways), finding inverse of matrix i.e A^-1 by Gauss-Jordan Elimination method, Factorization into A = LU , Transpose of a matrix, Permutation matrix.

**Lecture 3:** Multiplication of matrix 4 ways: row *col , row * matrix, matrix * col, col * row( It's a special case ).
Inverse of a matrix ( A^-1 A = I ), if and only if A^-1 exist and finding inverse i.e A^-1 by gauss - Jordan Elimination method.
Also, A^-1 A = I = A A^-1, i.e A inverse * A is same as A * A inverse which is equal to identity matrix (I).

**Lecture 4:** Factorization of matrix also known as matrix decomposition Is a way of reducing a matrix into constituent parts and in the form A = LU ( where L is lower triangular matrix and U is upper triangular matrix). More details here: https://machinelearningmastery.com/introduction-to-matrix-decompositions-for-machine-learning/

Inverse of two matrix i.e AB is B^-1A^-1 AB = I.
To find transpose of a matrix we make row as a column and vice-versa.
In permutation matrix we exchange rows. Also, In permutatin matrix Inverse of a matrix is equal to its transpose ( P^-1 = P^T).

**Thought:** Getting some more grip on this subject as I am moving along.


# Day 06

**Today's Progress:** Learned about vector spaces and subspaces, column space and null space in linear algebra from @MIT OpenCourseWare lecture 5 and lecture 6.

Vectors Space has to be closed under Multiplication and addition of vectors which means in other words linear combination

How to create a subspace from a matrix?
Take the columns of a matrix and take there combinations or there linear combinations and you get the column space.

**Thought:** completed the vector spaces part of linear algebra.


# Day 07

**Today's Progress:** Learned about computing the nullspaces, Echelon form, complete solution of AX = B and rank of a matrix from MIT lecture 7 and lecture 8.
 

**Thought:** learning some advance stuff and getting it to the middle of linear algebra course.


# Day 08

**Today's Progress:** Learned about the Linear independence, basics and dimensions, rowspaces and Nullspace of A^T from the lecture 9 and 10 of the MIT open courseware.

**Thought:** The things which I learned today was new to me and I had never learned this in my school. There can be some topics from linear algebra which you might not know, so I encourage you all if you are learning mathematics or ML please check out this MIT Linear Algebra course link. The link is mentioned in Day 04.  

# Day 09

**Today's Progress:** Day 09: learned about Bases of new vector spaces, rank one matrix and basics of graphs in linear algebra from @MIT courseware lecture 11.

**Thought:** Watched only one video today because I was working on my internship web project.


# Day 10

**Today's Progress:**  Learned about the Graphs & Networks, Incidence matrix, proved Kirchoff's law in terms of a matrix and solved some sums related to what I have learned so far from the MIT open courseware lecture 12 and 13.

**Thought:** After solving the kirchoff's law in term of matrix form I think linear algebra can do anything. :stuck_out_tongue_winking_eye: :heart_eyes:


# Day 11

**Today's Progress:** Learned about the orthogonal vectors, why nullspaces are perpendicular to rowspaces, Nullspace of 'A' transpose dot 'A' is equal to Nullspace of 'A', projections, least square and projection matrix from the MIT OCW.

Lecture 14: Orthogonal vectors, why nullspaces are perpendicular to rowspaces, Nullspace of 'A' transpose dot 'A' is equal to Nullspace of 'A' i.e N(A^T A) = N(A).

Lecture 15: Projections & why we use projections, least square and projection matrix.

**Thought:** Watched 2 video lectures today.

# Day 12

**Today's Progress:** Continued the projection matrix part and solved some sums related to that from MIT OCW lecture 16 of Linear Algebra.

**Thought:** today watched only one lecture.


# Day 13

**Today's Progress:** Learned about the Orthogonal basis, orthogonal matrix, Gram Schmidt method, determinants, 10 properties of determinants from Linear Algebra course.

Lecture 17: Orthogonal basis, orthogonal matrix, Gram Schmidt method.

Lecture 18: Determinants & 10 properties of determinants.

**Thought:** Understood the determinants part and its properites.


# Day 14

**Today's Progress:** Learned about the Formula for determinant of A, cofactor formula, tridiagonal matrices, formula for A^-1, Cramer's rule and volume of the box.

Lecture 19: Formula for determinant of A, cofactor formula, tridiagonal matrices.

Lecture 20: Formula for A^-1, Cramer's rule and volume of the box.

**Thought:** learned about the other formulas of the determinants.

# Day 15

**Today's Progress:** Learned about Eigen values and Eigen vectors, det [A - Î»I] = 0, diagonalization of a matrix from MIT linear algebra lecture 21 & 22.

**Thought:** Understanding the concepts.


# Day 16

 
**Today's Progress:** Learned about the Markov matrices, steady state, Fourier series of projections, differential equation of du/dr = Au and exponential e^At of a matrix for linear algebra MIT OCW lecture 23 & 24.


**Thought:** Also be reading the book of Linear Algebra by Prof. Gilbert Strange from tomorrow because i think it will be great to actually ready and revise those previous learned topics to understand the concepts better. As while watching a video there are topics which i didn't understand or i have some doubts related to a particular topic so that will be cleared through reading those topics again.


# Day 17

**Today's Progress:** Learned about symmetric matrices, hermitian matrices and positive definite matrices from lecture 25 of linear algebra.

**Thought:** Exploring & learning more concepts from linear algebra.


# Day 18

**Today's Progress:** Today learned about the Complex no's (in vectors & matrices), Fourier transform, fast Fourier transform, positive definite matrices (tests), test for minimum (X^TA > 0) and ellipsoids in R^n from lecture 26 &27 of MIT OCW linear algebra.

Lecture 26: This lectured covered Complex no's (in vectors & matrices), Fourier transform, fast Fourier transform. 

Lecture 27: This lectured covered positive definite matrices (tests), test for minimum (X^TA > 0) and ellipsoids in R^n.


**Thought:** Learning by proving the things.



# Day 19 

**Today's Progress:** Learned about the similar matrices (jordan form) and singular value decomposition (SVD) from mit linear algebra course.

Lecture 28: This lectured covered similar matrices (jordan form).

Lecture 29: This lectured covered singular value decomposition (SVD).

**Thought:** Today learned 2 topics in detail


# Day 20 

**Today's Progress:** Learned about linear transformation in linear algebra.

**Thought:** watched only a single video.



# Day 21 

**Today's Progress:** Learned about change of basis and its application (compression of images), left inverse, right inverse & pseudo inverse in linear algebra.

**Thought:** Finally completed the Linear Algebra part and will start with calculus from tomorrow.


# Day 22 

**Today's Progress:** Watched the first 5 videos of the 3Blue1Brown Essence of Calculus series.


**Thought:** Will try to complete this video series within the next two days. 










